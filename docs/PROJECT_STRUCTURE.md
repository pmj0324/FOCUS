# FOCUS: Project Structure Guide

## ğŸ“‚ Clean Organization

The project is now organized into clear, functional modules:

### Core Modules

```
focus/
â”‚
â”œâ”€â”€ ğŸ¨ models/                # Neural network architectures
â”‚   â”œâ”€â”€ embeddings.py         # Time & condition embeddings
â”‚   â””â”€â”€ unet.py              # SimpleUNet architecture
â”‚
â”œâ”€â”€ ğŸ“¦ dataloaders/           # Data management
â”‚   â”œâ”€â”€ cosmology_dataset.py # PyTorch Dataset class
â”‚   â””â”€â”€ prepare_data.py      # Data preprocessing
â”‚
â”œâ”€â”€ ğŸŒŠ diffusion/             # Diffusion algorithms
â”‚   â”œâ”€â”€ schedules.py         # Noise schedules (linear/cosine/quadratic)
â”‚   â””â”€â”€ ddpm.py              # DDPM & DDIM implementations
â”‚
â”œâ”€â”€ ğŸƒ training/              # Training infrastructure
â”‚   â”œâ”€â”€ trainer.py           # DiffusionTrainer class
â”‚   â””â”€â”€ callbacks.py         # EarlyStopping, Checkpointing, Logging
â”‚
â”œâ”€â”€ ğŸ”§ utils/                 # Utility functions
â”‚   â”œâ”€â”€ normalization.py     # Data normalization/denormalization
â”‚   â”œâ”€â”€ visualization.py     # Plotting & visualization
â”‚   â””â”€â”€ power_spectrum.py    # Power spectrum analysis
â”‚
â”œâ”€â”€ ğŸ”® parameter_inference/   # Parameter inference tools
â”‚   â”œâ”€â”€ inference.py         # Model loading & sampling
â”‚   â””â”€â”€ sampling.py          # Parameter sampling
â”‚
â”œâ”€â”€ ğŸŒ€ flowmatching/          # Flow matching (future)
â”‚
â””â”€â”€ ğŸ“Š manifold_analysis/     # Manifold analysis (future)
```

### Experiment Management

```
tasks/                        # All experiments here
â”œâ”€â”€ experiment_01/           # Example experiment
â”‚   â”œâ”€â”€ config.yaml          # Experiment configuration
â”‚   â”œâ”€â”€ checkpoints/         # Model checkpoints (.pt files)
â”‚   â”œâ”€â”€ logs/                # Training logs
â”‚   â””â”€â”€ figs/                # Generated figures
â”‚
â”œâ”€â”€ train_experiment.py      # Training script
â””â”€â”€ inference_experiment.py  # Inference script
```

### Configuration

```
configs/
â””â”€â”€ default.yaml             # Default configuration template
```

### Scripts & Testing

```
scripts/
â”œâ”€â”€ visualize_data.py        # Data visualization
â”œâ”€â”€ test_diffusion.py        # Test diffusion process
â”œâ”€â”€ test_real_data.py        # Test with real data
â”œâ”€â”€ visualize_forward_process.py  # Visualize diffusion
â”œâ”€â”€ read_data.py             # Data reading utilities
â”œâ”€â”€ run_all.sh               # Run all preprocessing
â””â”€â”€ legacy/                  # Old code (for reference only)
    â”œâ”€â”€ dataset.py           # âŒ Replaced by dataloaders/
    â”œâ”€â”€ diffusion.py         # âŒ Replaced by diffusion/
    â”œâ”€â”€ model_simple.py      # âŒ Replaced by models/
    â”œâ”€â”€ utils.py             # âŒ Replaced by utils/
    â””â”€â”€ prepare_data.py      # âŒ Replaced by dataloaders/
```

### Root Files

```
â”œâ”€â”€ train.py                 # â­ Main training entry point
â”œâ”€â”€ inference.py             # â­ Main inference entry point
â”œâ”€â”€ setup.py                 # Package installation
â”œâ”€â”€ requirements.txt         # Dependencies
â”œâ”€â”€ README.md                # Main documentation
â”œâ”€â”€ QUICKSTART.md            # Quick start guide
â””â”€â”€ PROJECT_STRUCTURE.md     # This file
```

## ğŸš€ Quick Usage

### 1. Prepare Data
```bash
python dataloaders/prepare_data.py --data_dir ./Data --output_dir ./processed_data
```

### 2. Create New Experiment
```bash
mkdir -p tasks/my_exp/{checkpoints,logs,figs}
cp configs/default.yaml tasks/my_exp/config.yaml
# Edit tasks/my_exp/config.yaml as needed
```

### 3. Train
```bash
python train.py --config tasks/my_exp/config.yaml --exp_dir tasks/my_exp
```

### 4. Generate Samples
```bash
python inference.py --config tasks/my_exp/config.yaml --exp_dir tasks/my_exp
```

## ğŸ“ Code Organization Principles

### âœ… DO:
- Add new models to `models/`
- Add new schedules to `diffusion/schedules.py`
- Create experiments in `tasks/`
- Use YAML configs for all experiments
- Import from packages: `from models import SimpleUNet`

### âŒ DON'T:
- Use files in `scripts/legacy/` (old versions)
- Create files in root directory
- Hard-code configurations
- Mix experiment code with core modules

## ğŸ”„ Adding New Features

### New Model
```python
# models/my_model.py
class MyModel(nn.Module):
    ...

# models/__init__.py
from .my_model import MyModel
__all__ = [..., 'MyModel']
```

### New Noise Schedule
```python
# diffusion/schedules.py
class MySchedule(NoiseSchedule):
    def get_betas(self):
        ...
```

### New Experiment
```bash
mkdir -p tasks/new_exp/{checkpoints,logs,figs}
cp configs/default.yaml tasks/new_exp/config.yaml
# Edit config and run
python train.py --config tasks/new_exp/config.yaml --exp_dir tasks/new_exp
```

## ğŸ“¦ Installation as Package

```bash
pip install -e .
```

Then use anywhere:
```python
from models import SimpleUNet
from diffusion import GaussianDiffusion
from training import DiffusionTrainer
```

## ğŸ—‘ï¸ What Got Moved?

| Old Location | New Location | Status |
|-------------|--------------|--------|
| `dataset.py` | `dataloaders/cosmology_dataset.py` | âœ… Replaced |
| `diffusion.py` | `diffusion/ddpm.py` | âœ… Replaced |
| `model_simple.py` | `models/unet.py` | âœ… Replaced |
| `utils.py` | `utils/` package | âœ… Replaced |
| `prepare_data.py` | `dataloaders/prepare_data.py` | âœ… Replaced |
| Testing scripts | `scripts/` | âœ… Organized |

## ğŸ¯ Next Steps

1. âœ… Clean modular structure
2. âœ… YAML-based experiment management
3. â³ Add flow matching to `flowmatching/`
4. â³ Add transformer models to `models/`
5. â³ Extend `parameter_inference/`
6. â³ Add analysis to `manifold_analysis/`

---

**Everything is now organized and ready for development!** ğŸš€

