# FOCUS: Configuration Guide

## ğŸ“ YAML ì„¤ì • íŒŒì¼ êµ¬ì¡°

### 1. ë°ì´í„° ì„¤ì • (Data)

```yaml
data:
  data_dir: "processed"  # 'processed' ë‹¨ì¶•ì–´ ì‚¬ìš© ê°€ëŠ¥!
  # ë˜ëŠ” ì „ì²´ ê²½ë¡œ:
  # maps_path: "./processed_data/maps_normalized.npy"
  # params_path: "./processed_data/params_normalized.npy"
  
  train_split: 0.9       # Train/Val ë¹„ìœ¨
  num_workers: 4         # ë°ì´í„° ë¡œë”© ì›Œì»¤
  shuffle: true          # train/val random split (true/false)
```

### 2. ëª¨ë¸ ì„¤ì • (Model) - ëª¨ë“ˆí™”ëœ êµ¬ì¡°

```yaml
model:
  from: "models.unet.SimpleUNet"  # Import ê²½ë¡œ
  # ëª¨ë¸ë³„ ì¸ì
  in_channels: 1
  out_channels: 1
  cond_dim: 6
  base_channels: 64
  channel_mults: [1, 2, 4, 8]
  time_dim: 256
```

**ìƒˆ ëª¨ë¸ ì¶”ê°€ ì˜ˆì‹œ:**
```yaml
# DiT ëª¨ë¸ì„ ë§Œë“¤ì—ˆì„ ë•Œ
model:
  from: "models.dit.DiT"
  hidden_size: 384
  depth: 12
  num_heads: 6
```

### 3. ë””í“¨ì „ ì„¤ì • (Diffusion)

```yaml
diffusion:
  timesteps: 1000
  beta_start: 1.0e-4
  beta_end: 0.02
  schedule: "linear"  # linear, cosine, quadratic
```

### 4. í•™ìŠµ ì„¤ì • (Training)

#### ê¸°ë³¸ ì˜µì…˜
```yaml
training:
  batch_size: 2
  num_epochs: 200
  lr: 1.0e-4
  weight_decay: 1.0e-4
  cfg_prob: 0.1
  sample_every: 10
  gradient_clip: 1.0
```

#### Optimizer ì„ íƒ
```yaml
training:
  optimizer: "adamw"  # adamw, adam, sgd
```

#### Scheduler ì„ íƒ

**Plateau (ê¸°ë³¸ê°’)**
```yaml
training:
  scheduler:
    name: "plateau"
    factor: 0.5      # Learning rateë¥¼ 50% ê°ì†Œ
    patience: 3      # 3 epoch ê¸°ë‹¤ë¦¼
    min_lr: 1.0e-7   # ìµœì†Œ learning rate
```

**Cosine Annealing**
```yaml
training:
  scheduler:
    name: "cosine"
    T_max: 200       # Cosine period
    eta_min: 1.0e-6  # ìµœì†Œ learning rate
```

**Step LR**
```yaml
training:
  scheduler:
    name: "step"
    step_size: 50    # 50 epochë§ˆë‹¤
    gamma: 0.1       # 10% ê°ì†Œ
```

**Scheduler ì‚¬ìš© ì•ˆí•¨**
```yaml
training:
  scheduler:
    name: "none"  # ë˜ëŠ” scheduler: null
```

### 5. ìƒ˜í”Œë§ ì„¤ì • (Sampling)

```yaml
sampling:
  method: "ddim"  # ddim or ddpm
  ddim_timesteps: 50
  cfg_scale: 2.0
  eta: 0.0
```

### 6. í•˜ë“œì›¨ì–´ ì„¤ì •

```yaml
device: "cuda"  # cuda or cpu
```

## ğŸ¯ ì™„ì „í•œ ì˜ˆì‹œ

### ê¸°ë³¸ ì„¤ì •
```yaml
data:
  data_dir: "processed"
  train_split: 0.9
  num_workers: 4
  shuffle: true

model:
  from: "models.unet.SimpleUNet"
  in_channels: 1
  out_channels: 1
  cond_dim: 6
  base_channels: 64
  channel_mults: [1, 2, 4, 8]
  time_dim: 256

diffusion:
  timesteps: 1000
  beta_start: 1.0e-4
  beta_end: 0.02
  schedule: "linear"

training:
  batch_size: 2
  num_epochs: 200
  lr: 1.0e-4
  weight_decay: 1.0e-4
  cfg_prob: 0.1
  sample_every: 10
  gradient_clip: 1.0
  optimizer: "adamw"
  
  scheduler:
    name: "plateau"
    factor: 0.5
    patience: 3
    min_lr: 1.0e-7

sampling:
  method: "ddim"
  ddim_timesteps: 50
  cfg_scale: 2.0
  eta: 0.0

device: "cuda"
```

### DiT ëª¨ë¸ ì˜ˆì‹œ (ë¯¸ë˜)
```yaml
data:
  data_dir: "processed"
  shuffle: false  # Sequential split

model:
  from: "models.dit.DiT"
  hidden_size: 384
  depth: 12
  num_heads: 6

training:
  batch_size: 4
  num_epochs: 500
  optimizer: "adamw"
  
  scheduler:
    name: "cosine"
    T_max: 500
    eta_min: 1.0e-6
```

## ğŸš€ ì‚¬ìš© ë°©ë²•

### 1. ìƒˆ ì‹¤í—˜ ìƒì„±
```bash
mkdir -p tasks/my_exp/{checkpoints,logs,figs}
cp configs/default.yaml tasks/my_exp/config.yaml
```

### 2. ì„¤ì • ìˆ˜ì •
```bash
nano tasks/my_exp/config.yaml
# ë˜ëŠ” ì—ë””í„°ë¡œ ìˆ˜ì •
```

### 3. í•™ìŠµ ì‹¤í–‰
```bash
python train.py --config tasks/my_exp/config.yaml --exp_dir tasks/my_exp
```

## ğŸ’¡ Tips

1. **`processed` ë‹¨ì¶•ì–´**: `data_dir: "processed"` ë¡œ ê°„ë‹¨íˆ ì„¤ì • ê°€ëŠ¥
2. **ëª¨ë¸ êµì²´**: `from: "models.ìƒˆëª¨ë¸.í´ë˜ìŠ¤ëª…"` ìœ¼ë¡œ ì‰½ê²Œ êµì²´
3. **Scheduler ì„ íƒ**: ì„±ëŠ¥ì— ë”°ë¼ ì ì ˆíˆ ì„ íƒ
   - Plateau: ì•ˆì •ì , ìˆ˜ë ´ í™•ì¸ ê°€ëŠ¥
   - Cosine: ê¸´ í•™ìŠµì— ì¢‹ìŒ
   - Step: ë‹¨ìˆœí•œ ê°ì†Œ
4. **Shuffle ì˜µì…˜**: `shuffle: false` ë¡œ sequential split ê°€ëŠ¥

