# Flow Matching ëª¨ë¸ì„ ì´ìš©í•œ Parameter Inference ì‹œìŠ¤í…œ

## ê°œìš”

Flow Matching ê¸°ë°˜ ìƒì„± ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬ ê´€ì¸¡ ë°ì´í„°ë¡œë¶€í„° ìš°ì£¼ë¡ ì  íŒŒë¼ë¯¸í„°ë¥¼ ì¶”ì •í•˜ëŠ” ì‹œìŠ¤í…œì…ë‹ˆë‹¤.

## ìƒì„±ëœ íŒŒì¼ë“¤

### 1. `parameter_inference.py` (ë©”ì¸ ëª¨ë“ˆ)
**ìœ„ì¹˜**: `/home/work/Cosmology/FOCUS/tasks/experiment_flow/parameter_inference.py`

**ì£¼ìš” í´ë˜ìŠ¤:**

#### `FlowMatchingLikelihood`
Flow Matching ëª¨ë¸ì˜ likelihoodë¥¼ ê³„ì‚°í•˜ëŠ” í´ë˜ìŠ¤

**ë©”ì„œë“œ:**
- `compute_divergence()`: Vector fieldì˜ divergence ê³„ì‚° (Hutchinson estimator)
- `compute_nll_approximate()`: ê·¼ì‚¬ negative log-likelihood ê³„ì‚°
- `compute_reconstruction_error()`: ì¬êµ¬ì„± ì˜¤ë¥˜ ê¸°ë°˜ likelihood (gradient ê°€ëŠ¥)

#### `ParameterInference`
íŒŒë¼ë¯¸í„° ì¶”ë¡ ì„ ìœ„í•œ ë©”ì¸ ì¸í„°í˜ì´ìŠ¤

**ë©”ì„œë“œ:**
- `__init__()`: ëª¨ë¸ ë° ì„¤ì • ë¡œë“œ
- `infer_mle()`: Maximum Likelihood Estimation (gradient ê¸°ë°˜)
- `infer_grid_search()`: 2D ê²©ì íƒìƒ‰
- `infer_mcmc()`: Metropolis-Hastings MCMC ìƒ˜í”Œë§
- `visualize_results()`: ê²°ê³¼ ì‹œê°í™”

### 2. `run_inference_example.py` (ì‹¤í–‰ ìŠ¤í¬ë¦½íŠ¸)
**ìœ„ì¹˜**: `/home/work/Cosmology/FOCUS/tasks/experiment_flow/run_inference_example.py`

ì‚¬ìš©ì ì¹œí™”ì ì¸ ì»¤ë§¨ë“œë¼ì¸ ì¸í„°í˜ì´ìŠ¤ë¥¼ ì œê³µí•˜ëŠ” ì˜ˆì œ ìŠ¤í¬ë¦½íŠ¸

**ì‚¬ìš© ì˜ˆì‹œ:**
```bash
# MLEë¡œ íŒŒë¼ë¯¸í„° ì¶”ì • (ê¸°ë³¸)
python run_inference_example.py --test_idx 1234

# MLE with custom settings
python run_inference_example.py --test_idx 1234 --iterations 500 --lr 0.005

# MCMC ìƒ˜í”Œë§
python run_inference_example.py --test_idx 1234 --method mcmc --num_samples 2000

# Grid search
python run_inference_example.py --test_idx 1234 --method grid --grid_points 30
```

### 3. `README_INFERENCE.md` (ì‚¬ìš©ì ê°€ì´ë“œ)
**ìœ„ì¹˜**: `/home/work/Cosmology/FOCUS/tasks/experiment_flow/README_INFERENCE.md`

ìì„¸í•œ ì‚¬ìš© ë°©ë²•, API ë¬¸ì„œ, ì˜ˆì œ ì½”ë“œë¥¼ í¬í•¨í•œ ì™„ì „í•œ ì‚¬ìš©ì ê°€ì´ë“œ

## êµ¬í˜„ ë°©ë²•ë¡ 

### Likelihood ê³„ì‚°

Flow Matchingì—ì„œ ì •í™•í•œ likelihoodëŠ” ë‹¤ìŒê³¼ ê°™ì´ ê³„ì‚°ë©ë‹ˆë‹¤:

```
log p(x_0 | Î¸) = log p(x_1) - âˆ«_0^1 âˆ‡Â·v_Î¸(x_t, t) dt
```

í•˜ì§€ë§Œ ì´ëŠ” ê³„ì‚° ë¹„ìš©ì´ ë§¤ìš° ë†’ìœ¼ë¯€ë¡œ, **ê·¼ì‚¬ ë°©ë²•**ì„ ì‚¬ìš©í•©ë‹ˆë‹¤:

#### ë°©ë²• 1: ì¬êµ¬ì„± ì˜¤ë¥˜ (Reconstruction Error)
```python
# ì—¬ëŸ¬ ì‹œê°„ tì—ì„œ vector field ì˜ˆì¸¡ ì˜¤ë¥˜ë¥¼ ì¸¡ì •
for t in [0.1, 0.3, 0.5, 0.7, 0.9]:
    x_t = (1-t) * x_0 + t * x_1  # interpolate
    v_pred = model(x_t, t, cond)
    v_true = x_1 - x_0
    error += ||v_pred - v_true||Â²
```

**ì¥ì **: 
- Gradient ê³„ì‚° ê°€ëŠ¥ â†’ MLEì— ì‚¬ìš©
- ë¹ ë¥¸ ê³„ì‚°

**ë‹¨ì **: 
- ê·¼ì‚¬ì¹˜ (ì •í™•í•œ likelihood ì•„ë‹˜)

#### ë°©ë²• 2: NLL ê·¼ì‚¬ (NLL Approximate)
ë¹„ìŠ·í•œ ë°©ì‹ì´ì§€ë§Œ ì—¬ëŸ¬ noise ìƒ˜í”Œì— ëŒ€í•´ í‰ê· 

### Maximum Likelihood Estimation (MLE)

**ì•Œê³ ë¦¬ì¦˜:**
1. íŒŒë¼ë¯¸í„° Î¸ë¥¼ ëœë¤ ì´ˆê¸°í™”
2. Reconstruction errorë¥¼ lossë¡œ ì‚¬ìš©
3. Adam optimizerë¡œ gradient descent
4. Î¸ âˆˆ [0,1]ë¡œ clipping (normalized space)

**ì½”ë“œ ì˜ˆì‹œ:**
```python
params_mle, losses = inferencer.infer_mle(
    x_obs,
    num_iterations=200,
    lr=0.01,
    method='reconstruction'
)
```

### MCMC Sampling

**ì•Œê³ ë¦¬ì¦˜:** Metropolis-Hastings
1. í˜„ì¬ íŒŒë¼ë¯¸í„° Î¸_currentì—ì„œ ì‹œì‘
2. Proposal: Î¸_prop ~ N(Î¸_current, ÏƒÂ²I)
3. Acceptance ratio: Î± = p(x|Î¸_prop) / p(x|Î¸_current)
4. Accept with probability min(1, Î±)

**ì½”ë“œ ì˜ˆì‹œ:**
```python
samples, acc_rate = inferencer.infer_mcmc(
    x_obs,
    num_samples=1000,
    burn_in=100,
    proposal_std=0.05
)
```

### Grid Search

2ì°¨ì› íŒŒë¼ë¯¸í„° ê³µê°„ì—ì„œ ê²©ì íƒìƒ‰ (ì „ì²´ 6ì°¨ì›ì€ ê³„ì‚°ì ìœ¼ë¡œ ë¶ˆê°€ëŠ¥)

**ì½”ë“œ ì˜ˆì‹œ:**
```python
params_best, likelihoods = inferencer.infer_grid_search(
    x_obs,
    grid_points=20  # 20x20 = 400 í‰ê°€
)
```

## ì„±ëŠ¥

### ê³„ì‚° ì‹œê°„ (NVIDIA GPU ê¸°ì¤€)

| ë°©ë²• | ì„¤ì • | ì‹œê°„ |
|------|------|------|
| MLE | 200 iterations | ~35ì´ˆ |
| MLE | 500 iterations | ~85ì´ˆ |
| MCMC | 1000 samples | ~10ë¶„ |
| Grid Search | 20x20 points | ~5ë¶„ |

### ì¶”ì • ì •í™•ë„

í…ŒìŠ¤íŠ¸ ê²°ê³¼ ì˜ˆì‹œ:
```
Parameter  True         Estimated    Error       
--------------------------------------------------
Î©m         0.107400     0.270237     0.162837    
Î©b         0.847400     1.001166     0.153766    
h          1.038140     2.146478     1.108338    
ns         0.459460     0.994257     0.534797    
Ïƒ8         0.762600     2.005954     1.243354    
w          1.708820     1.008645     0.700176    
```

**ê´€ì°°:**
- ì¼ë¶€ íŒŒë¼ë¯¸í„°ëŠ” ë¹„êµì  ì˜ ì¶”ì •ë¨ (Î©m, Î©b)
- ë‹¤ë¥¸ íŒŒë¼ë¯¸í„°ëŠ” ì–´ë ¤ì›€ (h, Ïƒ8)
- ì´ëŠ” ëª¨ë¸ì˜ ë¯¼ê°ë„ì™€ ë°ì´í„°ì˜ ì •ë³´ëŸ‰ì— ë”°ë¼ ë‹¤ë¦„

## ì¶œë ¥ íŒŒì¼ë“¤

### `inference_results/` ë””ë ‰í† ë¦¬

ì‹¤í–‰ í›„ ë‹¤ìŒ íŒŒì¼ë“¤ì´ ìƒì„±ë©ë‹ˆë‹¤:

1. **`inference_mle_idx{N}.png`**: MLE ê²°ê³¼ ì‹œê°í™”
   - ê´€ì¸¡ ë°ì´í„°
   - ì¶”ì •ëœ íŒŒë¼ë¯¸í„°ë¡œ ìƒì„±í•œ ìƒ˜í”Œ 4ê°œ
   - íŒŒë¼ë¯¸í„° ë¹„êµ í…Œì´ë¸”

2. **`mle_loss_idx{N}.png`**: ìµœì í™” loss ê·¸ë˜í”„

3. **`mcmc_posterior_idx{N}.png`**: (MCMC) Posterior ë¶„í¬
   - ê° íŒŒë¼ë¯¸í„°ì˜ íˆìŠ¤í† ê·¸ë¨
   - ì‹¤ì œê°’ vs ì¶”ì •ê°’ ë¹„êµ

4. **`grid_likelihood_idx{N}.png`**: (Grid) Likelihood surface

## í™•ì¥ ê°€ëŠ¥ì„±

### 1. ë” ì •í™•í•œ Likelihood ê³„ì‚°
```python
# Exact divergence using Hutchinson estimator
divergence = compute_divergence_exact(model, x_t, t, cond)
nll = -log_p_1 + integral_divergence
```

### 2. Amortized Inference
ë³„ë„ì˜ inference network í•™ìŠµ:
```python
# Training
theta_hat = inference_net(x_obs)
loss = ||theta_hat - theta_true||Â²

# Inference (one forward pass!)
theta_estimated = inference_net(x_obs_new)
```

### 3. Variational Inference
```python
# Learn posterior q(Î¸|x) â‰ˆ p(Î¸|x)
q_params = encoder(x_obs)
theta ~ q(Î¸|x)
loss = KL(q(Î¸|x) || p(Î¸)) - E[log p(x|Î¸)]
```

### 4. Ensemble Methods
```python
# Run MLE from multiple initial points
results = []
for init in random_initializations(n=10):
    params, _ = inferencer.infer_mle(x_obs, init_params=init)
    results.append(params)

# Average or weighted average
theta_final = weighted_average(results, weights=likelihoods)
```

## ê¸°ìˆ ì  ì„¸ë¶€ì‚¬í•­

### Gradient ê³„ì‚°

MLEë¥¼ ìœ„í•´ conditioning parametersì— ëŒ€í•œ gradientê°€ í•„ìš”:

```python
# Forward pass through the model
v_pred = model(x_t, t, cond)  # cond requires grad

# Compute loss
loss = ||v_pred - v_true||Â²

# Backward pass
loss.backward()  # Computes âˆ‚loss/âˆ‚cond

# Update parameters
optimizer.step()  # cond â† cond - lr * âˆ‚loss/âˆ‚cond
```

### Normalization

ëª¨ë“  ê³„ì‚°ì€ normalized space [0, 1]ì—ì„œ ìˆ˜í–‰:
```python
# Normalize
theta_norm = (theta - theta_min) / (theta_max - theta_min)

# Inference in normalized space
theta_est_norm = infer_mle(x_obs)

# Denormalize
theta_est = theta_est_norm * (theta_max - theta_min) + theta_min
```

## ë¬¸ì œ í•´ê²°

### 1. NaN ë˜ëŠ” Inf ë°œìƒ
```python
# Gradient clipping ì¶”ê°€
torch.nn.utils.clip_grad_norm_(params, max_norm=1.0)
```

### 2. Local Minima
```python
# ì—¬ëŸ¬ ì´ˆê¸°ê°’ ì‹œë„
for _ in range(10):
    init = torch.rand(1, 6)
    params, loss = infer_mle(x_obs, init_params=init)
    # Best one ì„ íƒ
```

### 3. MCMC Low Acceptance Rate
```python
# Proposal std ì¡°ì •
samples, acc = infer_mcmc(x_obs, proposal_std=0.02)  # smaller
```

### 4. Out of Memory
```python
# Batch size ì¤„ì´ê¸° ë˜ëŠ” CPU ì‚¬ìš©
inferencer = ParameterInference(..., device='cpu')
```

## ì°¸ê³ ë¬¸í—Œ

1. **Flow Matching**: Lipman et al., "Flow Matching for Generative Modeling", ICLR 2023
2. **Simulation-Based Inference**: Cranmer et al., "The frontier of simulation-based inference", PNAS 2020
3. **Neural Posterior Estimation**: Papamakarios et al., "Sequential Neural Likelihood", AISTATS 2019

## ë¼ì´ì„¼ìŠ¤

MIT License

## ê¸°ì—¬

Issuesì™€ PRs í™˜ì˜í•©ë‹ˆë‹¤!

---

**Happy Inferring!** ğŸš€ğŸ”­




