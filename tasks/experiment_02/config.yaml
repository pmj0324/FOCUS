# Experiment 02: Large Model Configuration

experiment:
  name: "large_baseline"
  description: "Large model with 128 base channels, same as Experiment 01"
  created: "2024-10-28"

# Data - use absolute paths
data:
  maps_path: "/home/work/Cosmology/FOCUS/processed_data/maps_normalized.npy"
  params_path: "/home/work/Cosmology/FOCUS/processed_data/params_normalized.npy"
  train_split: 0.9
  num_workers: 4
  shuffle: true

# Model - Large architecture (128 base channels)
model:
  from: "SimpleUNet"
  in_channels: 1
  out_channels: 1
  cond_dim: 6
  base_channels: 128  # Large model, same as Experiment 01
  channel_mults: [1, 2, 4, 8]
  time_dim: 256

# Diffusion
diffusion:
  timesteps: 1000
  beta_start: 1.0e-4
  beta_end: 0.02
  schedule: "linear"

# Training
training:
  batch_size: 64  # Large batch size, same as Experiment 01
  num_epochs: 200
  lr: 1.0e-4
  weight_decay: 1.0e-4
  cfg_prob: 0.1  # 10% unconditional training for CFG
  sample_every: 1  # Sample every epoch
  gradient_clip: 1.0
  
  # Optimizer
  optimizer: "adamw"
  
  # Scheduler: Plateau with warmup
  scheduler:
    name: "plateau"
    factor: 0.3  # 70% decrease in LR
    patience: 2  # Wait 2 epochs before reducing
    min_lr: 1.0e-7
    warmup_epochs: 5  # 5 epoch warmup

# Sampling
sampling:
  method: "ddim"
  ddim_timesteps: 50
  cfg_scale: 2.0
  eta: 0.0

# Hardware
device: "cuda"

